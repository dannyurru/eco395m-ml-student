# Homework 1 (10 Points + 3 Bonus)
This is my homework #1 for eco395m-ml. I have the work for questions 1-3 uploaded in a pdf file called "ML-Homemwork 1.pdf"

For question 5, having a low k neighbors (for example, k = 1) minimizes error since the points it selects are closer to the estimate of observed points and tested points. If k is really high, then the distance between the point and neighbors is going to have a higher sum. Then, error will be higher. However, as we've discussed in class, bias emerges as we select points that are more convenient for us, or are closer to the trained and predicted value. At k=1, there is no MSE, but high bias since we just are using our observed data to predict it. However, if k is at the highest possible value, it will not really follow a good estimate or follow a statitstically significant pattern, and will have too high of error. A good estimate has a number of k neighbors that does not lead to too much bias or MSE. The perfect number is subjective, but we don't want to minimize MSE for observed data, but rather want to for test data. Therefore, a good k neighbors value would be where MSE for test data is minimized, and where there is no data leakage and enough data to test on.
